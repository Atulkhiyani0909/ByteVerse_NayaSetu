{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUpI1ITa0/JpyYBrW3PwLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atulkhiyani0909/ByteVerse_NayaSetu/blob/main/Chatbot_RAG/NyayaSetu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "-eWOcGMX7zxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet --upgrade \\\n",
        "    langchain langchain-google-genai pymongo langchain_community \\\n",
        "    langchain_pinecone pinecone-client langchain_huggingface \\\n",
        "    numpy transformers --force-reinstall"
      ],
      "metadata": {
        "id": "3C4p0Z1P76SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "ai7x-Rdj9RsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "EmnwCQwo-Fq1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = userdata.get('LANGCHAIN_TRACING_V2')\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = userdata.get('LANGCHAIN_ENDPOINT')\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['HUGGING_FACE_API_KEY']=userdata.get('HUGGING_FACE_API_KEY')\n",
        "\n",
        "os.environ['PINECONE_API_KEY']=userdata.get('PINECONE_API_KEY')\n",
        "os.environ['MONGO_URL']=userdata.get('MONGO_URL')"
      ],
      "metadata": {
        "id": "u6Mkkcng9BQc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "2ICThYfF9hyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional,Literal,Union\n",
        "from bson import ObjectId\n",
        "\n",
        "# Core ML and RAG Libraries\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pymongo import MongoClient\n",
        "from pydantic import BaseModel,Field\n",
        "from functools import lru_cache\n",
        "from langchain.load import dumps,loads"
      ],
      "metadata": {
        "id": "d0YMRrqu9NpQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=1)\n",
        "def get_llm():\n",
        "    return ChatGoogleGenerativeAI(model='gemini-2.0-flash',temperature=0)\n",
        "\n",
        "llm = get_llm()\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class language_detector(BaseModel):\n",
        "    language: str = Field(..., description=\"Detected Language\")\n",
        "    translated: str = Field(..., description=\"Translated to English\")\n",
        "\n",
        "def query_to_english(query: str,memory: List) -> dict:\n",
        "    \"\"\"Detects the language of the input query and translates it to English.\"\"\"\n",
        "\n",
        "    lan_example = '''{\n",
        "        \"language\": \"Hindi\",\n",
        "        \"translated\": \"Hello, how are you?\"\n",
        "    }'''\n",
        "\n",
        "    prompt = \"\"\"Translate the following query to clear English while preserving its context and intent.\n",
        "    If the query is ambiguous, you can rephrase it, but do not change its original meaning utilize this Chat history to rewrite\n",
        "    this ambigous query : {memory}\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Also, detect the language of the query and store it in \"language\".\n",
        "\n",
        "    Output should strictly follow this format:\n",
        "    {example}\n",
        "    \"\"\"\n",
        "\n",
        "    llm3 = get_llm().with_structured_output(language_detector)\n",
        "\n",
        "    trans_template = ChatPromptTemplate.from_template(\n",
        "        template=prompt,\n",
        "        partial_variables={\n",
        "            'example': lan_example,\n",
        "            'memory': memory\n",
        "            }\n",
        "    )\n",
        "\n",
        "    trans_chain = trans_template | llm3  # No need for StrOutputParser since output is structured\n",
        "    return trans_chain.invoke({'query': query}).model_dump()  # Ensure structured dict output"
      ],
      "metadata": {
        "id": "E6dJImQw9PKI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = [()]"
      ],
      "metadata": {
        "id": "ne6hmDwW9Hak"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_to_english(\"Meri police ne complain nahi darj karwai\",memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm31OcTr-bA6",
        "outputId": "60eb19a0-6edc-4b6d-e52e-684589bc3431"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'language': 'Hindi', 'translated': 'My police complaint was not registered'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "apO1GS3w9E3Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQr3M49xAMCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-V9y_jwOJiSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q0__YCj0KZHy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}