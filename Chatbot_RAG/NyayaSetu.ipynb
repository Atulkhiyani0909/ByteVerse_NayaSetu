{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJyvVrCFtVBhx+L3CLbeDR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atulkhiyani0909/ByteVerse_NayaSetu/blob/main/Chatbot_RAG/NyayaSetu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "-eWOcGMX7zxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet --upgrade \\\n",
        "    langchain langchain-google-genai pymongo langchain_community \\\n",
        "    langchain_pinecone pinecone-client langchain_huggingface \\\n",
        "    numpy transformers --force-reinstall"
      ],
      "metadata": {
        "id": "3C4p0Z1P76SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "ai7x-Rdj9RsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "EmnwCQwo-Fq1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = userdata.get('LANGCHAIN_TRACING_V2')\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = userdata.get('LANGCHAIN_ENDPOINT')\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['HUGGING_FACE_API_KEY']=userdata.get('HUGGING_FACE_API_KEY')\n",
        "\n",
        "os.environ['PINECONE_API_KEY']=userdata.get('PINECONE_API_KEY')\n",
        "os.environ['MONGO_URL']=userdata.get('MONGO_URL')"
      ],
      "metadata": {
        "id": "u6Mkkcng9BQc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "2ICThYfF9hyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional,Literal,Union\n",
        "from bson import ObjectId\n",
        "\n",
        "# Core ML and RAG Libraries\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pymongo import MongoClient\n",
        "from pydantic import BaseModel,Field\n",
        "from functools import lru_cache\n",
        "from langchain.load import dumps,loads"
      ],
      "metadata": {
        "id": "d0YMRrqu9NpQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=1)\n",
        "def get_llm():\n",
        "    return ChatGoogleGenerativeAI(model='gemini-2.0-flash',temperature=0)\n",
        "\n",
        "llm = get_llm()\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class language_detector(BaseModel):\n",
        "    language: str = Field(..., description=\"Detected Language\")\n",
        "    translated: str = Field(..., description=\"Translated to English\")\n",
        "\n",
        "def query_to_english(query: str,memory: List) -> dict:\n",
        "    \"\"\"Detects the language of the input query and translates it to English.\"\"\"\n",
        "\n",
        "    lan_example = '''{\n",
        "        \"language\": \"Hindi\",\n",
        "        \"translated\": \"Hello, how are you?\"\n",
        "    }'''\n",
        "\n",
        "    prompt = \"\"\"Translate the following query to clear English while preserving its context and intent.\n",
        "    If the query is ambiguous, you can rephrase it, but do not change its original meaning utilize this Chat history to rewrite\n",
        "    this ambigous query : {memory}\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Also, detect the language of the query and store it in \"language\".\n",
        "\n",
        "    Output should strictly follow this format:\n",
        "    {example}\n",
        "    \"\"\"\n",
        "\n",
        "    llm3 = get_llm().with_structured_output(language_detector)\n",
        "\n",
        "    trans_template = ChatPromptTemplate.from_template(\n",
        "        template=prompt,\n",
        "        partial_variables={\n",
        "            'example': lan_example,\n",
        "            'memory': memory\n",
        "            }\n",
        "    )\n",
        "\n",
        "    trans_chain = trans_template | llm3  # No need for StrOutputParser since output is structured\n",
        "    return trans_chain.invoke({'query': query}).model_dump()  # Ensure structured dict output"
      ],
      "metadata": {
        "id": "E6dJImQw9PKI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = [()]"
      ],
      "metadata": {
        "id": "ne6hmDwW9Hak"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_to_english(\"Meri police ne complain nahi darj karwai\",memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm31OcTr-bA6",
        "outputId": "60eb19a0-6edc-4b6d-e52e-684589bc3431"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'language': 'Hindi', 'translated': 'My police complaint was not registered'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TalkBack(BaseModel):\n",
        "    talkback: bool = Field(..., description=\"Talkback\")\n",
        "\n",
        "def should_talkback(query: str,memory: List) -> dict:\n",
        "    \"\"\"Should talkback or not\"\"\"\n",
        "\n",
        "    prompt = '''\n",
        "    As a legal assistant for NyayaSetu, analyze the user's query and history till now to determine if it requires clarification before providing an accurate legal response. Use these criteria to decide:\n",
        "    In case of wild/unexpected/weird query , the answer should True only\n",
        "\n",
        "    **Return `True` if the query:**\n",
        "    1. Lacks sufficient details about the problem (e.g., vague or overly general).\n",
        "    2. Uses ambiguous terms like \"what happens\", \"can I\", or \"what are my rights\" without specifying the context.\n",
        "    3. Combines multiple legal issues into one query (e.g., \"What can I do if my train is delayed and I have a dispute with the police?\").\n",
        "    4. Doesn't provide enough context about the situation (e.g., missing details like location, type of incident, or parties involved).\n",
        "\n",
        "    **Return `False` if the query:**\n",
        "    1. Clearly describes a single legal problem (e.g., \"What are my rights if police refuse to file an FIR?\").\n",
        "    2. Includes sufficient context about the situation (e.g., \"I was detained by RPF for ticketless travel; what can I do?\").\n",
        "    3. Can be directly mapped to a legal provision or process based on available information.\n",
        "\n",
        "    **Query Examples:**\n",
        "\n",
        "    Ambiguous: \"What happens if I have a problem with railway staff?\"\n",
        "    → `True`\n",
        "\n",
        "    Clear: \"What are my rights if RPF detains me for ticketless travel?\"\n",
        "    → `False`\n",
        "\n",
        "    Ambiguous: \"What can I do if police refuse to help me?\"\n",
        "    → `True`\n",
        "\n",
        "    Clear: \"How do I file a complaint against police misconduct during detention?\"\n",
        "    → `False`\n",
        "\n",
        "    **User Query:** {query}\n",
        "\n",
        "    **Chat History:** {memory}\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'memory': memory\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = get_llm().with_structured_output(TalkBack)\n",
        "\n",
        "    chain = template | llm\n",
        "    return chain.invoke({'query': query}).model_dump()['talkback']"
      ],
      "metadata": {
        "id": "apO1GS3w9E3Z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "should_talkback('I got slapped by railway worker',memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQr3M49xAMCE",
        "outputId": "52332116-af85-4c61-ae91-41e70c39e573"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "should_talkback('get me my rights as per railway',memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V9y_jwOJiSF",
        "outputId": "7fbd8077-7b78-424f-eb10-7fafc906a2c0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Talkback message"
      ],
      "metadata": {
        "id": "PhX9Qp3aRNgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def talkback(query: str,memory: List,language: str) -> str:\n",
        "\n",
        "    prompt = '''\n",
        "    You are an AI assistant for a legal platform called NyayaSetu. Your goal is to refine vague user queries by asking for more details to provide accurate legal guidance.\n",
        "\n",
        "    ## Context:\n",
        "    - The user query may lack details, making it difficult to provide precise legal advice.\n",
        "    - Use the chat history to understand the context and determine what information has already been provided.\n",
        "    - Your task is to ask a single, logical follow-up question to clarify the user's intent or gather missing details.\n",
        "    - Keep the follow-up question concise, polite, and relevant to the query.\n",
        "\n",
        "    ## Chat History:\n",
        "    {chat_history}\n",
        "\n",
        "    ## User Query:\n",
        "    {query}\n",
        "\n",
        "    ## Response Format:\n",
        "    - Reply in language as specified by the user in chat history (if available),secondary to {language} otherwise default to English.\n",
        "    - Provide only one follow-up question that helps clarify the query or gather additional details.\n",
        "    - Ensure the response feels conversational and engaging.\n",
        "\n",
        "    ## Example Responses:\n",
        "    1. **User Query:** \"What happens if I have a problem with railway staff?\"\n",
        "    **AI:** \"Could you clarify whether this is about ticket disputes, harassment by staff, or refusal to address complaints?\"\n",
        "\n",
        "    2. **User Query:** \"Police won’t help with my complaint.\"\n",
        "    **AI:** \"Could you describe the issue in more detail? For example, is this about filing an FIR or addressing police misconduct?\"\n",
        "\n",
        "    3. **User Query:** \"What are my rights if RPF detains me?\"\n",
        "    **AI:** \"Could you provide more context? For instance, were you detained for ticketless travel or another issue?\"\n",
        "\n",
        "    4. **User Query:** \"Can I get compensation for a train delay?\"\n",
        "    **AI:** \"Could you specify how long the train was delayed and whether you had a reserved ticket?\"\n",
        "\n",
        "    Reply with only the follow-up question, nothing else.\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'chat_history':memory,\n",
        "            'language':language\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = get_llm()\n",
        "\n",
        "    chain = template | llm | StrOutputParser()\n",
        "    return chain.invoke({'query': query})"
      ],
      "metadata": {
        "id": "q0__YCj0KZHy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "talkback('I got hit by police officer',memory,'Hindi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QWav709meWKs",
        "outputId": "2acefa46-6919-4628-9f9a-251889bffc2b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Could you please provide more details about the incident, such as when and where it happened?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F0_wwbPBf991"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}